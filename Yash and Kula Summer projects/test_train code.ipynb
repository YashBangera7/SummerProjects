{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vari1= ig.open('/Users/kulasekharmaganti/SummerProjects/Yash and Kula Summer projects/emotion detection dataset/test/angry/im87.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=48x48 at 0x7FD9D06B27D0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vari1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = vari1.getdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ImagingCore at 0x7fd9d8d55bf0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr = np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2304,)\n"
     ]
    }
   ],
   "source": [
    "print(img_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_arr = img_arr/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8156862745098039"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(ig_arr)\n",
    "np.max(ig_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['opencv_frame_4.png',\n",
       " 'opencv_frame_4_actual.png',\n",
       " 'webcamimagecapture.ipynb',\n",
       " 'bgkulaact.png',\n",
       " 'opencv_frame_2.png',\n",
       " 'emotion detection dataset',\n",
       " '.DS_Store',\n",
       " 'opencv_frame_3.png',\n",
       " 'bag kula.png',\n",
       " 'opencv_frame_1.png',\n",
       " 'opencv_frame_0.png',\n",
       " 'opencv_frame_1_actual.png',\n",
       " 'test_train code.ipynb',\n",
       " 'PILimage.ipynb',\n",
       " 'opencv_frame_3_actual.png',\n",
       " 'opencv_frame_0_actual.png',\n",
       " 'grayscale_pixelresize.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'opencv_frame_2_actual.png',\n",
       " 'bgkula.png']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.listdir('emotion detection dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = 'emotion detection dataset' + '/' + path[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = 'emotion detection dataset' + '/' + path[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'emotion detection dataset/test'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happy',\n",
       " '.DS_Store',\n",
       " 'sad',\n",
       " 'fearful',\n",
       " 'neutral',\n",
       " 'angry',\n",
       " 'disgusted',\n",
       " 'surprised']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = os.listdir(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes.remove(\".DS_Store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happy', 'sad', 'fearful', 'neutral', 'angry', 'disgusted', 'surprised']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(n_classes):\n",
    "    temp_path = path_train + '/' + classes[i]\n",
    "    current_image_path = os.listdir(temp_path)\n",
    "    for j in range(len(current_image_path)):\n",
    "        if current_image_path[j]=='.DS_Store':\n",
    "            pass\n",
    "        else:\n",
    "            current_image = np.array(ig.open(temp_path + '/' + current_image_path[j]).getdata())\n",
    "            X_train.append(current_image)\n",
    "            y_train.append(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_classes):\n",
    "    temp_path = path_test + '/' + classes[i]\n",
    "    current_image_path = os.listdir(temp_path)\n",
    "    for j in range(len(current_image_path)):\n",
    "        if current_image_path[j]=='.DS_Store':\n",
    "            pass\n",
    "        else:\n",
    "            current_image = np.array(ig.open(temp_path + '/' + current_image_path[j]).getdata())\n",
    "            X_test.append(current_image)\n",
    "            y_test.append(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7178"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7178"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28709"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28709"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happy', 'sad', 'fearful', 'neutral', 'angry', 'disgusted', 'surprised']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_new = []\n",
    "for i in range(len(y_train)):\n",
    "    if (y_train[i]=='happy'):\n",
    "        y_train_new.append(0)\n",
    "    elif (y_train[i]=='sad'):\n",
    "        y_train_new.append(1)\n",
    "    elif (y_train[i]=='fearful'):\n",
    "        y_train_new.append(2)\n",
    "    elif (y_train[i]=='neutral'):\n",
    "        y_train_new.append(3)\n",
    "    elif (y_train[i]=='angry'):\n",
    "        y_train_new.append(4)\n",
    "    elif (y_train[i]=='disgusted'):\n",
    "        y_train_new.append(5)\n",
    "    elif (y_train[i]=='surprised'):\n",
    "        y_train_new.append(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_new = []\n",
    "for i in range(len(y_test)):\n",
    "    if (y_test[i]=='happy'):\n",
    "        y_test_new.append(0)\n",
    "    elif (y_test[i]=='sad'):\n",
    "        y_test_new.append(1)\n",
    "    elif (y_test[i]=='fearful'):\n",
    "        y_test_new.append(2)\n",
    "    elif (y_test[i]=='neutral'):\n",
    "        y_test_new.append(3)\n",
    "    elif (y_test[i]=='angry'):\n",
    "        y_test_new.append(4)\n",
    "    elif (y_test[i]=='disgusted'):\n",
    "        y_test_new.append(5)\n",
    "    elif (y_test[i]=='surprised'):\n",
    "        y_test_new.append(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after one-hot encoding:  (28709, 7)\n"
     ]
    }
   ],
   "source": [
    "Y_train = np_utils.to_categorical(y_train_new, n_classes)\n",
    "Y_test = np_utils.to_categorical(y_test_new, n_classes)\n",
    "print(\"Shape after one-hot encoding: \", Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28709"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape = ()\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape = ()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(2304)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7178"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the input vector from the 48x48 pixels\n",
    "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = Sequential()\\n# hidden layer\\nmodel.add(Dense(100, input_shape=(2304,), activation='relu'))\\n# output layer\\nmodel.add(Dense(7, activation='softmax'))\\n\\n# looking at the model summary\\nmodel.summary()\\n# compiling the sequential model\\nmodel.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\\n# training the model for 10 epochs\\nmodel.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_test, Y_test))\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model = Sequential()\n",
    "# hidden layer\n",
    "model.add(Dense(100, input_shape=(2304,), activation='relu'))\n",
    "# output layer\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "# looking at the model summary\n",
    "model.summary()\n",
    "# compiling the sequential model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "# training the model for 10 epochs\n",
    "model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_test, Y_test))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# ADDED FIRST CONVOLUTION LAYER\\nmodel = Sequential()\\n# convolutional layer\\nmodel.add(Conv2D(25, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu', input_shape=(48,48,1)))\\nmodel.add(MaxPool2D(pool_size=(1,1)))\\n# flatten output of conv\\nmodel.add(Flatten())\\n# hidden layer\\nmodel.add(Dense(100, activation='relu'))\\n# output layer\\nmodel.add(Dense(7, activation='softmax'))\\n\\n# compiling the sequential model\\nmodel.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\\n\\n# training the model for 10 epochs\\nmodel.fit(X_train, Y_train, batch_size=128, epochs=45, validation_data=(X_test, Y_test))\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# ADDED FIRST CONVOLUTION LAYER\n",
    "model = Sequential()\n",
    "# convolutional layer\n",
    "model.add(Conv2D(25, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu', input_shape=(48,48,1)))\n",
    "model.add(MaxPool2D(pool_size=(1,1)))\n",
    "# flatten output of conv\n",
    "model.add(Flatten())\n",
    "# hidden layer\n",
    "model.add(Dense(100, activation='relu'))\n",
    "# output layer\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "# compiling the sequential model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# training the model for 10 epochs\n",
    "model.fit(X_train, Y_train, batch_size=128, epochs=45, validation_data=(X_test, Y_test))'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 219s 971ms/step - loss: 1.7614 - accuracy: 0.2784 - val_loss: 1.6115 - val_accuracy: 0.3546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd9ba4e5210>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ADDED MORE CONVOLUTION LAYER\n",
    "# building a linear stack of layers with the sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# convolutional layer\n",
    "model.add(Conv2D(50, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu', input_shape=(48,48,1)))\n",
    "\n",
    "# convolutional layer\n",
    "model.add(Conv2D(75, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(125, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# flatten output of conv\n",
    "model.add(Flatten())\n",
    "\n",
    "# hidden layer\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "# output layer\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "# compiling the sequential model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# training the model for 10 epochs\n",
    "model.fit(X_train, Y_train, batch_size=128, epochs=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "yashhappy= ig.open('/Users/kulasekharmaganti/SummerProjects/Yash and Kula Summer projects/emotion detection dataset/test/fearful/im17.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['opencv_frame_4_actual.png',\n",
       " 'opencv_frame_1_actual.png',\n",
       " 'opencv_frame_3_actual.png',\n",
       " 'opencv_frame_0_actual.png',\n",
       " 'opencv_frame_2_actual.png']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAAAAAByaaZbAAAFiklEQVR4nEWUy65eRxGF16ru/V/PxcfBSWziYCMSh4AUMUiQEAjlBTJADMiUB+AheAukPANSpiiZImZICSCRCUmMkX1sn+t/23t3rcXgPwKpR6Wu7qr6PhU/7bp5V0sIsAEiggGAgNqu36yzRBdkKaWovyoVtmzbsE2AMI0oZKKUKAVRgGANrM6e37tfCaUUDfD+XQDAs7+yfnBbUSIUUSI47l48ryfLrgJsEQkAoAESfPrHr15cDe/99kcDyFIisFu/uJw/uJsb8NNaatSODpIEWKL94fO6hfO13z+43m0UHq5fbE7uH1cP23tVCiBBgoww6Xj+7PBMBk8/+d1BsPVX577zaBl5Ma1wtSRCETD3XdvKTAL1L49+s96+fDm598rE276V0lk1C53FGaIhBoD59UtPc+SYl/L05PC45Grk/AgjndUWi8UsNBQm/Odv29GsL23EFqUs2TbqDjoIpluFFUKERcImefXZ0C3ckY2Pd0EwJjPaAGS0gGFJAmTAkicPc1K6Oj06LP9+VoJYTvZsYSBDe8iyAQBSLt5Xiel0fvvtZRtBGiRgk5BatRWmkdyDtrEr5Xoy352edOMCNAwDEcGpnRU3EhG2goDdl0W/Kle5c62gTAQCbqah+j/xbMM00Z5Uq9bZOFTSGOgA0qA02SeIiv11OExNZ2XeHx18e0gVm3Uw4bbd7ib30CphCwAt2nCAaBOWbv78rbO6bJYyd/2x1/OToVmhG6stSzbA/mwaZRa8e7E9KhbUztvU89ePKtKtZggUwxZhWuhz4bJ7ur2jUQ3oMLnjlBOwkUEIkgVYaRhoO7r1a9fUUrLtlCxITmfIlmWnAFiwvN151+bloo2HRbYFu2VmurkFbMuZ1g1zL16hMtEuBz2CLUvy/jS0UO4hyDdBlQ8/TkwXCwzTN0bJtlJKSZBaVYhG7MGJNtvxjDleN/f3X2nKZEjOhKiEAjeUrZvfE3lyvCvz4t3dItmZVjqlzFEZMAwTsgRbaeX8Z8M4SPlu5v9r3XeiEPbFGbYk2VJ7/2TIHKaHeROQMjMzpawWEAIiaYFwGjl//YzirA9mJkJOJRudzdUK2IBCABWiLN4+7GMx/WI+OYSdKWWB4CyqokAKFEFRYMrs5rVDOf+8O7lzb56ZVmPCmaiwCAEBgRTBLKsvv+40G+dvPq7nF0+/f1dSsshQKnyzvdNKSUrlN599MaCG3/7lXDjgP79VSmpSSgzQvpnFfnsMX/4jX+2j1umHt98cF93F+vTaylTLzObQXq69AlLLvz9576O3xjovH/zg1Z9/r16uZnmRma1lk5JhGdIN5EyffvPug9mdxax7+NGduny4Wk07n/WZqcyWDaHc65j7hPb1O/dd3nrUXv34jXraL0Z3k5V2mZlOWa6iM5AOBABclZ9cFcx//eUvZpPrYXkb6lZjDHXvSE5YVfbcYJvG8+N+e8L69ju7J9vH3cNJKq6ONgfTFBrYJqU6HSBDUCHbsx+fXB3cQt/lk/NvPjD6uq3NQ8vIFNvfDqsFmzRJR6yvDy92m8mMzafrW7dy0MwxdGOmnMLqbFfToIJmkCqXO/+pf+enJXa69+RuGZ+3kiPK5ihpCRd9idRe2/3gVtv+V2/+cF6kEd+99PZfUQBu+qamzLY+fVLlKAAZDsPr/uvvlGOij3G9oy//c+DCcXVbiZYeN2vUkR1CYVMgrofLozemZRznaS983gegsyXSgtUPk6h9YXE4YCJ2vdtkGhzJ2VyvtYuxIs8nB00pW73d6qa6qwqToK9Tm+Hl8bTNm27NDr/ajpO2mhxAmabUF/R1aCYYQZC4UD1fT3Iz9aiT/vHlKG5j2V0t8mYpTfs60lPSjQxoleH1bIkyDh2fPj54EeKy7liUDqd4dP1fznsGm/R/tvAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=48x48 at 0x7FD99A405490>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ig.open(array_str[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1_array = []\n",
    "for i in range(len(array)):\n",
    "    img1_array.append(np.array(ig.open(array_str[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[177, 182, 187, ..., 187, 173, 176],\n",
       "        [177, 181, 186, ..., 145, 171, 202],\n",
       "        [178, 181, 184, ..., 158, 182, 207],\n",
       "        ...,\n",
       "        [108, 112, 120, ...,  68,  38,  30],\n",
       "        [102, 104, 111, ...,  72,  45,  31],\n",
       "        [ 97,  98, 103, ...,  73,  59,  32]], dtype=uint8),\n",
       " array([[177, 182, 188, ..., 186, 173, 176],\n",
       "        [177, 182, 186, ..., 146, 172, 202],\n",
       "        [177, 181, 184, ..., 159, 182, 207],\n",
       "        ...,\n",
       "        [107, 110, 118, ...,  68,  39,  29],\n",
       "        [101, 102, 109, ...,  71,  45,  30],\n",
       "        [ 97,  97, 103, ...,  73,  60,  32]], dtype=uint8),\n",
       " array([[177, 181, 186, ..., 186, 172, 178],\n",
       "        [177, 181, 186, ..., 145, 172, 203],\n",
       "        [177, 181, 186, ..., 159, 181, 206],\n",
       "        ...,\n",
       "        [108, 112, 119, ...,  69,  38,  29],\n",
       "        [101, 103, 110, ...,  71,  45,  30],\n",
       "        [ 96,  99, 104, ...,  74,  60,  33]], dtype=uint8),\n",
       " array([[178, 180, 187, ..., 187, 173, 176],\n",
       "        [177, 181, 187, ..., 146, 171, 202],\n",
       "        [177, 182, 185, ..., 159, 182, 207],\n",
       "        ...,\n",
       "        [107, 111, 119, ...,  68,  38,  30],\n",
       "        [102, 103, 108, ...,  72,  45,  30],\n",
       "        [ 97,  97, 102, ...,  74,  60,  32]], dtype=uint8),\n",
       " array([[177, 181, 186, ..., 187, 172, 177],\n",
       "        [178, 182, 186, ..., 146, 172, 202],\n",
       "        [178, 182, 184, ..., 159, 182, 206],\n",
       "        ...,\n",
       "        [107, 112, 119, ...,  69,  38,  29],\n",
       "        [102, 103, 110, ...,  73,  46,  31],\n",
       "        [ 97,  97, 103, ...,  74,  60,  33]], dtype=uint8)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1_array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "img11_arr = X_test.reshape(X_test.shape[0], 48, 48, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7178, 48, 48, 1)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img11_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(array):\n",
    "    prediction = []\n",
    "    for i in range(len(array)):\n",
    "        if (np.argmax(array[i])==0):\n",
    "            prediction.append('happy')\n",
    "        elif (np.argmax(array[i])==1):\n",
    "            prediction.append('sad')\n",
    "        elif (np.argmax(array[i])==2):\n",
    "            prediction.append('fearful')\n",
    "        elif (np.argmax(array[i])==3):\n",
    "            prediction.append('neutral')\n",
    "        elif (np.argmax(array[i])==4):\n",
    "            prediction.append('angry')\n",
    "        elif (np.argmax(array[i])==5):\n",
    "            prediction.append('disgusted')\n",
    "        elif (np.argmax(array[i])==6):\n",
    "            prediction.append('surprised')\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_str = np.array(img1_array).reshape(np.array(img1_array).shape[0],48,48,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "array = model.predict(array_str)\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = prediction(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happy', 'happy', 'happy', 'happy', 'happy']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.02      0.23      0.04        88\n",
      "   disgusted       0.00      0.00      0.00        11\n",
      "     fearful       0.03      0.16      0.05       179\n",
      "       happy       0.31      0.44      0.37      1252\n",
      "     neutral       0.20      0.19      0.20      1271\n",
      "         sad       0.02      0.32      0.03        66\n",
      "   surprised       0.81      0.16      0.26      4311\n",
      "\n",
      "    accuracy                           0.21      7178\n",
      "   macro avg       0.20      0.21      0.13      7178\n",
      "weighted avg       0.58      0.21      0.26      7178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(predictions,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opencv_frame_0.png written!\n",
      "opencv_frame_1.png written!\n",
      "opencv_frame_2.png written!\n",
      "opencv_frame_3.png written!\n",
      "opencv_frame_4.png written!\n",
      "Escape hit, closing...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "cv2.namedWindow(\"test\")\n",
    "\n",
    "img_counter = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print(\"failed to grab frame\")\n",
    "        break\n",
    "    cv2.imshow(\"test\", frame)\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256 == 27:\n",
    "        # ESC pressed\n",
    "        print(\"Escape hit, closing...\")\n",
    "        break\n",
    "    elif k%256 == 32:\n",
    "        # SPACE pressed\n",
    "        img_name = \"opencv_frame_{}.png\".format(img_counter)\n",
    "        cv2.imwrite(img_name, frame)\n",
    "        print(\"{} written!\".format(img_name))\n",
    "        img_counter += 1\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyWindow(\"test\")\n",
    "cv2.waitKey(1)\n",
    "\n",
    "import glob\n",
    "\n",
    "array = glob.glob('opencv_frame_?.png')\n",
    "from PIL import Image as ig\n",
    "import cv2\n",
    "for i in range(len(array)):\n",
    "    image = cv2.imread(array[i])\n",
    "    current_value = array[i].split('_')[-1].split('.')[0]\n",
    "    gray_img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    cv2.imwrite('bag kula.png', gray_img)\n",
    "\n",
    "    kula1=ig.open('./bag kula.png')\n",
    "    bgkula=kula1.resize((48,48))\n",
    "\n",
    "    bgkula.save('opencv_frame_{}_actual.png'.format(current_value))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_str = glob.glob('opencv_frame_?_actual.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['opencv_frame_4_actual.png',\n",
       " 'opencv_frame_1_actual.png',\n",
       " 'opencv_frame_3_actual.png',\n",
       " 'opencv_frame_0_actual.png',\n",
       " 'opencv_frame_2_actual.png']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
